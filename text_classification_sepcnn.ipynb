{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text-classification-sepcnn.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNeMDTrjwrq4bdqTzRmpy8y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mariammaher550/text-classification-sepcnn/blob/main/text_classification_sepcnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "check labels , 0 is alcohol"
      ],
      "metadata": {
        "id": "uS-tST-FqoP3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1dnniK4LqoDM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fyiT0tYqZUR9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import nltk\n",
        "from keras_preprocessing import text, sequence\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "top_k = 20000\n",
        "max_sequence_length = 500"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_set():\n",
        "\n",
        "    df = pd.read_csv('items_cat_processed_data.csv')\n",
        "    data_text = df['title'].values.tolist()  # input\n",
        "    data_labels = df['label'].values.tolist()  # output\n",
        "\n",
        "    data_labels = np.array(data_labels)\n",
        "\n",
        "    for i in range (len(data_text)):\n",
        "        data_text[i] = str(data_text[i])\n",
        "\n",
        "\n",
        "    train_text, test_text, train_labels, test_labels = train_test_split(data_text, data_labels, test_size=0.33)\n",
        "\n",
        "    return ((train_text, train_labels), (test_text, test_labels))\n",
        "\n",
        "\n",
        "def sequence_vectorization(train_text, validate_text):\n",
        "\n",
        "\n",
        "    tokenizer = text.Tokenizer(num_words=top_k)\n",
        "    tokenizer.fit_on_texts(train_text)\n",
        "\n",
        "    x_train = tokenizer.texts_to_sequences(train_text)\n",
        "    x_val = tokenizer.texts_to_sequences(validate_text)\n",
        "\n",
        "    max_length = len(max(x_train, key=len))\n",
        "    if max_length > max_sequence_length:\n",
        "        max_length = max_sequence_length\n",
        "\n",
        "    x_train = sequence.pad_sequences(x_train, maxlen=max_length)\n",
        "    x_val = sequence.pad_sequences(x_val, maxlen=max_length)\n",
        "\n",
        "    return x_train, x_val, tokenizer.word_index\n",
        "\n",
        "\n",
        "def get_num_classes(labels):\n",
        "\n",
        "    num_classes = max(labels) + 1\n",
        "    missing_classes = [i for i in range(num_classes) if i not in labels]\n",
        "    if len(missing_classes):\n",
        "        raise ValueError('Missing samples with label value(s) '\n",
        "                         '{missing_classes}. Please make sure you have '\n",
        "                         'at least one sample for every label value '\n",
        "                         'in the range(0, {max_class})'.format(\n",
        "                            missing_classes=missing_classes,\n",
        "                            max_class=num_classes - 1))\n",
        "\n",
        "    if num_classes <= 1:\n",
        "        raise ValueError('Invalid number of labels: {num_classes}.'\n",
        "                         'Please make sure there are at least two classes '\n",
        "                         'of samples'.format(num_classes=num_classes))\n",
        "    return num_classes"
      ],
      "metadata": {
        "id": "eESL4g2DZp3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import models\n",
        "from keras.layers import Embedding, Dropout, SeparableConv1D, MaxPooling1D, GlobalAveragePooling1D, Dense\n",
        "\n",
        "\n",
        "def sepcnn_model(blocks,\n",
        "                 filters,\n",
        "                 kernel_size,\n",
        "                 embedding_dim,\n",
        "                 dropout_rate,\n",
        "                 pool_size,\n",
        "                 input_shape,\n",
        "                 num_classes,\n",
        "                 num_features, ):\n",
        "  \n",
        "    op_units, op_activation = _get_last_layer_units_and_activation(num_classes)\n",
        "    model = models.Sequential()\n",
        "\n",
        "    model.add(Embedding(input_dim=num_features,\n",
        "                        output_dim=embedding_dim,\n",
        "                        input_length=input_shape[0]))\n",
        "\n",
        "    for _ in range(blocks - 1):\n",
        "        model.add(Dropout(rate=dropout_rate))\n",
        "        model.add(SeparableConv1D(filters=filters,\n",
        "                                  kernel_size=kernel_size,\n",
        "                                  activation='relu',\n",
        "                                  bias_initializer='random_uniform',\n",
        "                                  depthwise_initializer='random_uniform',\n",
        "                                  padding='same'))\n",
        "        model.add(SeparableConv1D(filters=filters,\n",
        "                                  kernel_size=kernel_size,\n",
        "                                  activation='relu',\n",
        "                                  bias_initializer='random_uniform',\n",
        "                                  depthwise_initializer='random_uniform',\n",
        "                                  padding='same'))\n",
        "        model.add(MaxPooling1D(pool_size=pool_size))\n",
        "\n",
        "    model.add(SeparableConv1D(filters=filters * 2,\n",
        "                              kernel_size=kernel_size,\n",
        "                              activation='relu',\n",
        "                              bias_initializer='random_uniform',\n",
        "                              depthwise_initializer='random_uniform',\n",
        "                              padding='same'))\n",
        "    model.add(SeparableConv1D(filters=filters * 2,\n",
        "                              kernel_size=kernel_size,\n",
        "                              activation='relu',\n",
        "                              bias_initializer='random_uniform',\n",
        "                              depthwise_initializer='random_uniform',\n",
        "                              padding='same'))\n",
        "    model.add(GlobalAveragePooling1D())\n",
        "    model.add(Dropout(rate=dropout_rate))\n",
        "    model.add(Dense(op_units, activation=op_activation))\n",
        "    return model\n",
        "\n",
        "\n",
        "def _get_last_layer_units_and_activation(num_classes):\n",
        "    if num_classes == 2:\n",
        "        activation = \"sigmoid\"\n",
        "        units = 1\n",
        "    else:\n",
        "        activation = 'softmax'\n",
        "        units = num_classes\n",
        "\n",
        "    return units, activation\n"
      ],
      "metadata": {
        "id": "4YC2E6uUZvep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train_sequence_model(data,\n",
        "                         learning_rate=1e-3,\n",
        "                         epochs=1000,\n",
        "                         batch_size=128,\n",
        "                         blocks=2,\n",
        "                         filters=64,\n",
        "                         dropout_rate=0.2,\n",
        "                         embedding_dim=200,\n",
        "                         kernel_size=3,\n",
        "                         pool_size=3):\n",
        "\n",
        "    (train_texts, train_labels), (val_texts, val_labels) = data\n",
        "\n",
        "\n",
        "    num_classes = get_num_classes(train_labels)\n",
        "    unexpected_labels = [v for v in val_labels if v not in range(num_classes)]\n",
        "    if len(unexpected_labels):\n",
        "        raise ValueError('Unexpected label values found in the validation set:'\n",
        "                         ' {unexpected_labels}. Please make sure that the '\n",
        "                         'labels in the validation set are in the same range '\n",
        "                         'as training labels.'.format(\n",
        "                             unexpected_labels=unexpected_labels))\n",
        "\n",
        "    x_train, x_val, word_index = sequence_vectorization(\n",
        "            train_texts, val_texts)\n",
        "\n",
        "    num_features = min(len(word_index) + 1, top_k)\n",
        "\n",
        "    model = sepcnn_model(blocks=blocks,\n",
        "                                     filters=filters,\n",
        "                                     kernel_size=kernel_size,\n",
        "                                     embedding_dim=embedding_dim,\n",
        "                                     dropout_rate=dropout_rate,\n",
        "                                     pool_size=pool_size,\n",
        "                                     input_shape=x_train.shape[1:],\n",
        "                                     num_classes=num_classes,\n",
        "                                     num_features=num_features)\n",
        "\n",
        "    loss = 'sparse_categorical_crossentropy'\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])\n",
        "\n",
        "    callbacks = [tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss', patience=2)]\n",
        "\n",
        "    history = model.fit(\n",
        "            x_train,\n",
        "            train_labels,\n",
        "            epochs=epochs,\n",
        "            callbacks=callbacks,\n",
        "            validation_data=(x_val, val_labels),\n",
        "            verbose=2,  # Logs once per epoch.\n",
        "            batch_size=batch_size)\n",
        "\n",
        "    # Print results\n",
        "    history = history.history\n",
        "    prediction = model.predict(val_texts[:10])\n",
        "    tested = val_texts[:10]\n",
        "    print('Validation accuracy: {acc}, loss: {loss}'.format(\n",
        "            acc=history['val_acc'][-1], loss=history['val_loss'][-1]))\n",
        "\n",
        "    # Save model.\n",
        "    model.save('sepcnn_model.h5')\n",
        "\n",
        "    return history['val_acc'][-1], history['val_loss'][-1]\n",
        "\n"
      ],
      "metadata": {
        "id": "MNS8RRLqZ1Kr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = prepare_set()\n",
        "train_sequence_model(data)\n"
      ],
      "metadata": {
        "id": "7T_2nbPpN2Db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "050dc4ac-0a48-4517-bbf8-a0d045f149dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "133867 133867\n",
            "Epoch 1/1000\n",
            "701/701 - 56s - loss: 2.0588 - acc: 0.3182 - val_loss: 1.2754 - val_acc: 0.5694 - 56s/epoch - 80ms/step\n",
            "Epoch 2/1000\n",
            "701/701 - 54s - loss: 0.8335 - acc: 0.7614 - val_loss: 0.6552 - val_acc: 0.8231 - 54s/epoch - 77ms/step\n",
            "Epoch 3/1000\n",
            "701/701 - 56s - loss: 0.5237 - acc: 0.8609 - val_loss: 0.5264 - val_acc: 0.8641 - 56s/epoch - 79ms/step\n",
            "Epoch 4/1000\n",
            "701/701 - 54s - loss: 0.3806 - acc: 0.9012 - val_loss: 0.4815 - val_acc: 0.8785 - 54s/epoch - 76ms/step\n",
            "Epoch 5/1000\n",
            "701/701 - 53s - loss: 0.3118 - acc: 0.9191 - val_loss: 0.4845 - val_acc: 0.8797 - 53s/epoch - 75ms/step\n",
            "Epoch 6/1000\n",
            "701/701 - 53s - loss: 0.2685 - acc: 0.9299 - val_loss: 0.5050 - val_acc: 0.8805 - 53s/epoch - 75ms/step\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 18) for input KerasTensor(type_spec=TensorSpec(shape=(None, 18), dtype=tf.float32, name='embedding_2_input'), name='embedding_2_input', description=\"created by layer 'embedding_2_input'\"), but it was called on an input with incompatible shape (None,).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-7e0e5ed6ad60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_sequence_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-19-7d87cad64ed8>\u001b[0m in \u001b[0;36mtrain_sequence_model\u001b[0;34m(data, learning_rate, epochs, batch_size, blocks, filters, dropout_rate, embedding_dim, kernel_size, pool_size)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;31m# Print results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_texts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0mtested\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_texts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     print('Validation accuracy: {acc}, loss: {loss}'.format(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1148\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1801, in predict_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1790, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1783, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1751, in predict_step\n        return self(x, training=False)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/input_spec.py\", line 214, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" '\n\n    ValueError: Exception encountered when calling layer \"sequential_2\" (type Sequential).\n    \n    Input 0 of layer \"separable_conv1d_8\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 200)\n    \n    Call arguments received:\n      • inputs=tf.Tensor(shape=(None,), dtype=string)\n      • training=False\n      • mask=None\n"
          ]
        }
      ]
    }
  ]
}